import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader, random_split
from collections import Counter

from MSIDataset import MSIDataset, list_files, load_sample, train_model
from config import *

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.encoding = self._generate_positional_encoding(d_model, max_len)

    def forward(self, x):
        return x + self.encoding[:, :x.size(1), :].to(x.device)
    
    def _generate_positional_encoding(self, d_model, max_len):
        """Generate the positional encoding matrix."""
        d_model = d_model - (d_model % 2) 
        encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        encoding[:, 0::2] = torch.sin(position * div_term)
        encoding[:, 1::2] = torch.cos(position * div_term)
        return encoding.unsqueeze(0)

class TransformerClassifier(nn.Module):
    def __init__(self, input_dim=INPUT_DIM, model_dim=MODEL_DIM, num_heads=NUM_HEADS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES, chunk_size=CHUNK_SIZE, seq_len=FIXED_SEQ_LEN):
        super(TransformerClassifier, self).__init__()
        self.chunk_size = chunk_size
        self.model_dim = model_dim - (model_dim % 2) 
        self.seq_len = seq_len
        
        self.embedding = nn.Linear(input_dim, self.model_dim)
        self.positional_encoding = PositionalEncoding(d_model=self.model_dim, max_len=chunk_size)
        
        encoder_layer = nn.TransformerEncoderLayer(d_model=self.model_dim, nhead=num_heads, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.num_chunks = (seq_len + chunk_size - 1) // chunk_size
        
        self.fc1 = nn.Linear(self.num_chunks * self.model_dim, 64)
        self.bn1 = nn.BatchNorm1d(64)
        self.fc2 = nn.Linear(64, num_classes)
        
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        chunked_outputs = []
        
        for start in range(0, seq_len, self.chunk_size):
            end = min(start + self.chunk_size, seq_len)
            x_chunk = x[:, start:end, :]
            x_chunk = self.embedding(x_chunk)
            x_chunk = self.positional_encoding(x_chunk)
            x_chunk = self.transformer_encoder(x_chunk)
            chunked_outputs.append(torch.mean(x_chunk, dim=1))
        
        x = torch.cat(chunked_outputs, dim=1)
        
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x


def main():
    global train_dataset, val_dataset

    encoded_files = list_files(ENCODED_DIR, '.csv')
    classifications_files = list_files(CLASSIFICATIONS_DIR, '.cifc')

    load_sample(encoded_files, classifications_files)

    dataset = MSIDataset(ENCODED_DIR, CLASSIFICATIONS_DIR, CHUNK_SIZE, FIXED_SEQ_LEN)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    # Train the final model with the best hyperparameters
    best_model = TransformerClassifier(input_dim=INPUT_DIM, 
                                       model_dim=MODEL_DIM, 
                                       num_heads=NUM_HEADS,
                                       num_layers=NUM_LAYERS,
                                       num_classes=NUM_CLASSES,
                                       chunk_size=CHUNK_SIZE,
                                       seq_len=FIXED_SEQ_LEN)
    
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

    train_model(best_model, train_dataloader, val_dataloader, 
                learning_rate=LEARNING_RATE,
                weight_decay=WEIGHT_DECAY)


if __name__ == '__main__':
    main()