import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import sys
import warnings
from trainTransformer import TransformerClassifier, PositionalEncoding
from MSIDataset import load_sample
from config import *

def filter_torch_warnings(message, category, filename, lineno, file=None, line=None):
    if "Torch was not compiled with flash attention" in str(message):
        return None
    return message


warnings.showwarning = filter_torch_warnings

def load_model(model_path):
    # Determine the available device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model = TransformerClassifier(
        input_dim=INPUT_DIM, 
        model_dim=MODEL_DIM, 
        num_heads=NUM_HEADS,
        num_layers=NUM_LAYERS,
        num_classes=NUM_CLASSES,
        chunk_size=CHUNK_SIZE,
        seq_len=FIXED_SEQ_LEN
    )
    
    loaded_model = torch.load(model_path, map_location=device)
    
    if isinstance(loaded_model, nn.Module):
        # If a full model was saved
        model = loaded_model
    elif isinstance(loaded_model, dict):
        # If only the state dict was saved
        model.load_state_dict(loaded_model)
    else:
        raise TypeError(f"Unexpected type for loaded model: {type(loaded_model)}")
    
    model.to(device)
    model.eval()
    return model, device

def preprocess_input(file_path, device):
    with open(file_path, 'r') as file:
        next(file)
        data = np.loadtxt(file, delimiter=',')

    if data.shape[0] < FIXED_SEQ_LEN:
        padded_data = np.zeros((FIXED_SEQ_LEN, data.shape[1]))
        padded_data[:data.shape[0], :] = data
        data = padded_data
    else:
        data = data[:FIXED_SEQ_LEN, :]

    return torch.FloatTensor(data).unsqueeze(0).to(device)

def run_inference(model, input_tensor):
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = F.softmax(output, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    return predicted_class, confidence

def get_class_label(predicted_class):
    return "Malware" if predicted_class == 1 else "Clean"

def main():
    if len(sys.argv) != 3:
        print("Usage: python inference.py <model_path> <input_file>")
        sys.exit(1)

    model_path = sys.argv[1]
    input_file = sys.argv[2]

    try:
        model, device = load_model(model_path)
        print(f"Using device: {device}")

        input_tensor = preprocess_input(input_file, device)

        predicted_class, confidence = run_inference(model, input_tensor)

        class_label = get_class_label(predicted_class)

        print(f"Predicted class: {class_label}")
        print(f"Confidence: {confidence:.4f}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()