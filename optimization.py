import optuna
import traceback
import seaborn as sns
from trainTransformer import TransformerClassifier
from MSIDataset import MSIDataset, list_files, load_sample, validate_model, train_model
from torch.utils.data import Dataset, DataLoader, random_split
import torch
import torch.nn as nn
import torch.optim as optim
from config import *
import matplotlib.pyplot as plt
import pandas as pd

def objective(trial):
    try:
        model_dim_head_multiplier = trial.suggest_int('model_dim_head_multiplier', 2, 16, step=2)  # Ensure model_dim is always even
        num_heads = trial.suggest_int('num_heads', 2, 8)
        model_dim = num_heads * model_dim_head_multiplier
        num_layers = trial.suggest_int('num_layers', 1, 6)
        chunk_size = trial.suggest_int('chunk_size', 100, 1000, step=100)
        batch_size = trial.suggest_int('batch_size', 32, 128)
        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)

        model_dim = model_dim - (model_dim % num_heads)

        available_vram = check_vram()
        estimated_model_size = model_dim * num_layers * 4  
        if available_vram < estimated_model_size * 1.5: 
            print(f"Not enough VRAM. Available: {available_vram}, Estimated needed: {estimated_model_size * 1.5}")
            return float('inf') 

        model = TransformerClassifier(input_dim=INPUT_DIM, model_dim=model_dim, num_heads=num_heads, 
                                      num_layers=num_layers, num_classes=NUM_CLASSES, chunk_size=chunk_size,
                                      seq_len=FIXED_SEQ_LEN)

        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

        best_val_loss = float('inf')
        for epoch in range(NUM_EPOCHS):
            model.train()
            total_loss = 0
            accumulation_steps = 4

            for i, (batch_data, batch_labels, _) in enumerate(train_dataloader):
                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)
                outputs = model(batch_data)
                loss = criterion(outputs, batch_labels)
                loss = loss / accumulation_steps  
                loss.backward()
                
                if (i+1) % accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()

                total_loss += loss.item()

            val_loss, _, _, _, _ = validate_model(model, val_dataloader, criterion, device)
            
            if val_loss < best_val_loss:
                best_val_loss = val_loss

            trial.report(val_loss, epoch)

            if trial.should_prune():
                raise optuna.TrialPruned()

        return best_val_loss

    except RuntimeError as e:
        if "CUDA out of memory" in str(e):
            print(f"CUDA out of memory error. Skipping this trial.")
            return float('inf')
        elif "must match the existing size" in str(e):
            print(f"Dimension mismatch error. Skipping this trial.")
            return float('inf')
        else:
            print(f"Unexpected error occurred: {str(e)}")
            traceback.print_exc()
            return float('inf')

def visualize_optuna_results(study):
    """
    Visualize the results of an Optuna study.
    
    Args:
    study (optuna.study.Study): The completed Optuna study object.
    """

    fig = plt.figure(figsize=(20, 15))

    ax1 = fig.add_subplot(221)
    optuna.visualization.matplotlib.plot_optimization_history(study)
    ax1.set_title("Optimization History")

    ax2 = fig.add_subplot(222)
    optuna.visualization.matplotlib.plot_param_importances(study)
    ax2.set_title("Parameter Importances")

    ax3 = fig.add_subplot(223)
    optuna.visualization.matplotlib.plot_parallel_coordinate(study)
    ax3.set_title("Parallel Coordinate Plot")

    ax4 = fig.add_subplot(224)
    param_importances = optuna.importance.get_param_importances(study)
    top_params = list(param_importances.keys())[:2] 
    
    df = pd.DataFrame({
        top_params[0]: [t.params[top_params[0]] for t in study.trials],
        top_params[1]: [t.params[top_params[1]] for t in study.trials],
        'value': [t.value for t in study.trials]
    })
    
    scatter = ax4.scatter(df[top_params[0]], df[top_params[1]], c=df['value'], cmap='viridis')
    ax4.set_xlabel(top_params[0])
    ax4.set_ylabel(top_params[1])
    ax4.set_title(f"Objective Value for Top 2 Parameters")
    plt.colorbar(scatter, ax=ax4, label='Objective Value')

    plt.tight_layout()
    plt.show()

    param_names = list(study.best_params.keys())
    df_params = pd.DataFrame([t.params for t in study.trials])
    df_params['value'] = [t.value for t in study.trials]
    
    sns.pairplot(df_params, vars=param_names, hue='value', diag_kind='kde', plot_kws={'alpha': 0.6})
    plt.suptitle("Pairwise Relationships Between Parameters", y=1.02)
    plt.show()

    print("Best parameters:", study.best_params)
    print("Best value:", study.best_value)

def check_vram():
    if torch.cuda.is_available():
        device = torch.cuda.current_device()
        total_memory = torch.cuda.get_device_properties(device).total_memory
        allocated_memory = torch.cuda.memory_allocated(device)
        available_memory = total_memory - allocated_memory
        return available_memory
    return 0 

def main():
    global train_dataset, val_dataset

    encoded_files = list_files(ENCODED_DIR, '.csv')
    classifications_files = list_files(CLASSIFICATIONS_DIR, '.cifc')

    load_sample(encoded_files, classifications_files)

    dataset = MSIDataset(ENCODED_DIR, CLASSIFICATIONS_DIR, CHUNK_SIZE, FIXED_SEQ_LEN)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=25, catch=(RuntimeError,))

    visualize_optuna_results(study)

    print('Number of finished trials: {}'.format(len(study.trials)))
    print('Best trial:')
    trial = study.best_trial

    print('  Value: {}'.format(trial.value))
    print('  Params: ')
    for key, value in trial.params.items():
        print('    {}: {}'.format(key, value))

    best_model = TransformerClassifier(input_dim=INPUT_DIM, 
                                       model_dim=trial.params['model_dim_head_multiplier']*trial.params['num_heads'], 
                                       num_heads=trial.params['num_heads'],
                                       num_layers=trial.params['num_layers'],
                                       num_classes=NUM_CLASSES,
                                       chunk_size=trial.params['chunk_size'],
                                       seq_len=FIXED_SEQ_LEN)
    
    train_dataloader = DataLoader(train_dataset, batch_size=trial.params['batch_size'], shuffle=True, num_workers=4)
    val_dataloader = DataLoader(val_dataset, batch_size=trial.params['batch_size'], shuffle=False, num_workers=4)

    train_model(best_model, train_dataloader, val_dataloader, 
                learning_rate=trial.params['learning_rate'],
                weight_decay=trial.params['weight_decay'])


if __name__ == '__main__':
    main()