import os
import torch
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from torchvision import transforms
from trainTransformer import MSIDataset, TransformerClassifier, PositionalEncoding
from trainLSTM import LSTMClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from LSTM_config import *

def preprocess_data(data, input_dim):
    """
    A function to preprocess data if needed.
    Modify this function based on the specific requirements of your model.
    """
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5] * input_dim, std=[0.5] * input_dim)
    ])
    data = transform(data)
    return data

def load_model(model_path):
    """
    Load the trained model.
    """
    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    device = torch.device('cpu')
    try:
        model = torch.load(model_path)
        model.to(device)
    
    except AttributeError:
        model = LSTMClassifier(input_dim=INPUT_DIM, 
                                hidden_dim=MODEL_DIM, 
                                num_layers=NUM_LAYERS,
                                num_classes=NUM_CLASSES,
                                chunk_size=CHUNK_SIZE,
                                seq_len=FIXED_SEQ_LEN)
    
        model.load_state_dict(torch.load(model_path))
    
    model.eval() 
    return model

def run_inference(model, dataloader):
    """
    Run the model on the data and print the outputs.
    """
    all_predictions = []
    all_labels = []
    all_files = []
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    with torch.no_grad():  
        for data, label, file_name in dataloader:
            outputs = model(data)
            _, predicted = torch.max(outputs.data, 1)
            
            all_predictions.extend(predicted.numpy())
            all_labels.extend(label.numpy())
            all_files.extend(file_name)

    return all_predictions, all_labels, all_files

def calculate_statistics(predictions, labels, files):
    """
    Calculate and print accuracy, false positives, false negatives, 
    and accuracy for each class/category. Also create visualizations.
    """

    overall_accuracy = accuracy_score(labels, predictions)

    exe_indices = [i for i, file in enumerate(files) if ".exe" in file.lower()]
    exe_labels = [labels[i] for i in exe_indices]
    exe_predictions = [predictions[i] for i in exe_indices]
    exe_accuracy = accuracy_score(exe_labels, exe_predictions)
    exe_fp = sum((exe_predictions[i] == 1 and exe_labels[i] == 0) for i in range(len(exe_predictions)))
    exe_fp_rate = exe_fp / len(exe_indices)
    
    msi_indices = [i for i, file in enumerate(files) if ".msi" in file.lower()]
    msi_labels = [labels[i] for i in msi_indices]
    msi_predictions = [predictions[i] for i in msi_indices]
    msi_accuracy = accuracy_score(msi_labels, msi_predictions)
    msi_fp = sum((msi_predictions[i] == 1 and msi_labels[i] == 0) for i in range(len(msi_predictions)))
    msi_fp_rate = msi_fp / len(msi_indices)
    
    false_positives = sum((predictions[i] == 1 and labels[i] == 0) for i in range(len(predictions)))
    false_negatives = sum((predictions[i] == 0 and labels[i] == 1) for i in range(len(predictions)))

    class_accuracies = {}
    for i, file in enumerate(files):
        class_file_path = f'./data/categories/{os.path.splitext(os.path.basename(file))[0]}.class'
        
        if os.path.exists(class_file_path):
            with open(class_file_path, 'r') as f:
                class_name = f.read().strip()
                
                if class_name not in class_accuracies:
                    class_accuracies[class_name] = {"labels": [], "predictions": []}
                
                class_accuracies[class_name]["labels"].append(labels[i])
                class_accuracies[class_name]["predictions"].append(predictions[i])

    for class_name, data in class_accuracies.items():
        class_accuracy = accuracy_score(data["labels"], data["predictions"])
        print(f'Accuracy for class "{class_name}": {class_accuracy:.4f} for {len(data["labels"])} files')
    
    print(f'\nOverall Accuracy: {overall_accuracy:.4f}')
    print(f'EXE Accuracy: {exe_accuracy:.4f} for {len(exe_indices)} files')
    print(f'MSI Accuracy: {msi_accuracy:.4f} for {len(msi_indices)} files')
    print(f'EXE False Positive Rate: {exe_fp_rate:.4f}')
    print(f'MSI False Positive Rate: {msi_fp_rate:.4f}')
    print(f'Overall False Positives (predicted 1, actual 0): {false_positives}')
    print(f'Overall False Negatives (predicted 0, actual 1): {false_negatives}')

    plot_confusion_matrix(labels, predictions)
    plot_class_accuracies(class_accuracies)
    plot_msi_exe_comparison(exe_accuracy, msi_accuracy, exe_fp_rate, msi_fp_rate)


def plot_msi_exe_comparison(exe_accuracy, msi_accuracy, exe_fp_rate, msi_fp_rate):
    """
    Plot bar chart comparing MSI and EXE accuracies and false positive rates.
    """
    categories = ['Accuracy', 'False Positive Rate']
    exe_values = [exe_accuracy, exe_fp_rate]
    msi_values = [msi_accuracy, msi_fp_rate]

    x = np.arange(len(categories))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, exe_values, width, label='EXE')
    rects2 = ax.bar(x + width/2, msi_values, width, label='MSI')

    ax.set_ylabel('Rate')
    ax.set_title('Comparison of EXE and MSI Files')
    ax.set_xticks(x)
    ax.set_xticklabels(categories)
    ax.legend()

    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.annotate(f'{height:.2f}',
                        xy=(rect.get_x() + rect.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom')

    autolabel(rects1)
    autolabel(rects2)

    fig.tight_layout()
    plt.savefig('msi_exe_comparison.png')
    plt.close()

def plot_confusion_matrix(y_true, y_pred):
    """
    Plot confusion matrix.
    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.savefig('confusion_matrix.png')
    plt.close()

def plot_class_accuracies(class_accuracies):
    """
    Plot bar chart of class accuracies.
    """
    classes = list(class_accuracies.keys())
    accuracies = [accuracy_score(data["labels"], data["predictions"]) for data in class_accuracies.values()]
    
    plt.figure(figsize=(12, 6))
    plt.bar(classes, accuracies)
    plt.title('Accuracy by Class')
    plt.ylabel('Accuracy')
    plt.xlabel('Class')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig('class_accuracies.png')
    plt.close()

def plot_msi_exe_comparison(exe_accuracy, msi_accuracy, exe_fp_rate, msi_fp_rate):
    """
    Plot bar chart comparing MSI and EXE accuracies and false positive rates.
    """
    categories = ['Accuracy', 'False Positive Rate']
    exe_values = [exe_accuracy, exe_fp_rate]
    msi_values = [msi_accuracy, msi_fp_rate]

    x = np.arange(len(categories))
    width = 0.35

    fig, ax = plt.subplots(figsize=(10, 6))
    rects1 = ax.bar(x - width/2, exe_values, width, label='EXE')
    rects2 = ax.bar(x + width/2, msi_values, width, label='MSI')

    ax.set_ylabel('Rate')
    ax.set_title('Comparison of EXE and MSI Files')
    ax.set_xticks(x)
    ax.set_xticklabels(categories)
    ax.legend()

    def autolabel(rects):
        for rect in rects:
            height = rect.get_height()
            ax.annotate(f'{height:.2f}',
                        xy=(rect.get_x() + rect.get_width() / 2, height),
                        xytext=(0, 3), 
                        textcoords="offset points",
                        ha='center', va='bottom')

    autolabel(rects1)
    autolabel(rects2)

    fig.tight_layout()
    plt.savefig('msi_exe_comparison.png')
    plt.close()

def main():
    # Paths to directories and model
    encoded_dir = './data/encoded'
    classifications_dir = './data/classifications'
    model_path = './LSTM_model_exe.pth'

    # Hyperparameters (adjust as necessary)
    chunk_size = 1000
    fixed_seq_len = 1000
    batch_size = 128
    input_dim = 512

    dataset = MSIDataset(encoded_dir, classifications_dir, chunk_size=chunk_size, fixed_seq_len=fixed_seq_len)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    model = load_model(model_path)

    predictions, labels, files = run_inference(model, dataloader)

    for file, prediction, label in zip(files, predictions, labels):
        print(f'File: {file}, Prediction: {prediction}, Ground Truth: {label}')

    calculate_statistics(predictions, labels, files)

    print("\nVisualizations have been saved as 'confusion_matrix.png', 'class_accuracies.png', and 'msi_exe_comparison.png'.")

if __name__ == "__main__":
    main()